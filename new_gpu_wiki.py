#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ —Å—Ç–∞—Ç–µ–π Wikipedia —Å –ø–æ–º–æ—â—å—é Gemma3:27 —á–µ—Ä–µ–∑ Ollama
"""

import pandas as pd
import json
import requests
import time
import logging
from typing import Dict, List, Optional
import os
from tqdm import tqdm

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('qa_generation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class WikipediaQAGenerator:
    def __init__(self, ollama_url: str = "http://localhost:11434", model: str = "gemma3:27b"):
        self.ollama_url = ollama_url
        self.model = model
        self.session = requests.Session()

    def test_ollama_connection(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Ollama"""
        try:
            response = self.session.get(f"{self.ollama_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                if self.model in model_names:
                    logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama —É—Å–ø–µ—à–Ω–æ. –ú–æ–¥–µ–ª—å {self.model} –¥–æ—Å—Ç—É–ø–Ω–∞.")
                    return True
                else:
                    logger.error(f"‚ùå –ú–æ–¥–µ–ª—å {self.model} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {model_names}")
                    return False
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama: {e}")
            return False

    def generate_qa_from_text(self, text: str) -> Optional[Dict[str, str]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏"""
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        cleaned_text = text.strip().replace('"', '').replace('\n', ' ')

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if len(cleaned_text) > 5000:
            cleaned_text = cleaned_text[:5000] + "..."

        prompt = f"""Generate a natural question-answer pair in Kyrgyz language for training data collection.

        Text: {cleaned_text}

        Instructions:
        1. Analyze the content and create a broad, contextual question about the main topic:
           - Focus on the SUBJECT/THEME rather than specific objects ("this book", "this document")
           - Ask about general concepts, processes, or categories mentioned
           - Avoid questions that assume specific context unknown to the reader

        2. Question generation logic:
           - If content lists items ‚Üí "What are the main [category] in [field/area]?"
           - If content explains process ‚Üí "How does [process] work?"
           - If content describes concept ‚Üí "What is [concept]?"
           - If content gives overview ‚Üí "What can you tell about [topic area]?"

        3. Answer requirements:
           - Transform the original text into a natural response
           - Remove any references to specific documents/sources
           - Make minimal grammatical corrections only
           - Present information as general knowledge, not as "this text says"

        4. Language requirements:
           - Use only Kyrgyz language
           - Make both question and answer sound completely natural
           - Never reference source ("–±—É–ª –∫–∏—Ç–µ–ø—Ç–µ", "–¥–æ–∫—É–º–µ–Ω—Ç—Ç–µ", "—Ç–µ–∫—Å—Ç–µ")
           - Ensure proper Kyrgyz grammar and sentence flow

        5. Example transformations:
           - Bad: "–ë—É–ª –∫–∏—Ç–µ–ø—Ç–µ –∫–∞–Ω–¥–∞–π —ã—Ä—á—ã–ª–∞—Ä –±–∞—Ä?" 
           - Good: "–ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω–¥—ã–Ω –±–µ–ª–≥–∏–ª“Ø“Ø –∫–æ–º–ø–æ–∑–∏—Ç–æ—Ä–ª–æ—Ä—É –∂–∞–Ω–∞ —ã—Ä—á—ã–ª–∞—Ä—ã –∫–∏–º–¥–µ—Ä?"

        Return response in strict JSON format:
        {{
            "question": "your natural question here",
            "answer": "your comprehensive answer here"
        }}"""

        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "max_tokens": 8000
                }
            }

            response = self.session.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=300
            )

            if response.status_code == 200:
                result = response.json()
                generated_text = result.get('response', '').strip()

                # –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞
                qa_pair = self.parse_qa_response(generated_text)
                return qa_pair
            else:
                logger.error(f"–û—à–∏–±–∫–∞ API Ollama: {response.status_code} - {response.text}")
                return None

        except requests.exceptions.Timeout:
            logger.error("–¢–∞–π–º-–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama: {e}")
            return None
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            return None

    def parse_qa_response(self, response_text: str) -> Optional[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            cleaned_response = response_text.strip()

            # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ JSON –≤ –æ—Ç–≤–µ—Ç–µ
            json_start = cleaned_response.find('{')
            json_end = cleaned_response.rfind('}') + 1

            if json_start != -1 and json_end > json_start:
                json_str = cleaned_response[json_start:json_end]
                try:
                    qa_data = json.loads(json_str)
                    if 'question' in qa_data and 'answer' in qa_data:
                        return {
                            "question": qa_data['question'].strip(),
                            "answer": qa_data['answer'].strip()
                        }
                except json.JSONDecodeError:
                    pass

            # Fallback: –ø–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Å–ª–µ–¥—É–µ—Ç –Ω–æ–≤–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É)
            lines = response_text.split('\n')
            question = None
            answer = None

            for line in lines:
                line = line.strip()
                if line.startswith('–°—É—Ä–æ–æ:'):
                    question = line.replace('–°—É—Ä–æ–æ:', '').strip()
                elif line.startswith('–ñ–æ–æ–ø:'):
                    answer = line.replace('–ñ–æ–æ–ø:', '').strip()
                    # –°–æ–±–∏—Ä–∞–µ–º –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç
                    idx = lines.index(line)
                    if idx < len(lines) - 1:
                        remaining_lines = lines[idx + 1:]
                        for remaining_line in remaining_lines:
                            remaining_line = remaining_line.strip()
                            if remaining_line and not remaining_line.startswith('–°—É—Ä–æ–æ:'):
                                answer += ' ' + remaining_line
                            else:
                                break

            if question and answer:
                return {
                    "question": question,
                    "answer": answer
                }
            else:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç: {response_text}")
                return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞: {e}")
            return None

    def process_csv_file(self, csv_file_path: str, output_file_path: str,
                         start_from: int = 0, max_articles: Optional[int] = None):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ CSV —Ñ–∞–π–ª–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ Q&A"""

        if not os.path.exists(csv_file_path):
            logger.error(f"CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_file_path}")
            return

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama
        if not self.test_ollama_connection():
            return

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞: {csv_file_path}")

        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞
            df = pd.read_csv(csv_file_path)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç–∞—Ç–µ–π")

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            df = df.dropna(subset=['Text'])  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç–∞—Ç—å–∏
            df = df[df['Text'].str.len() > 50]  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏ –¥–ª–∏–Ω–Ω–µ–µ 50 —Å–∏–º–≤–æ–ª–æ–≤

            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            qa_dataset = []
            processed_indices = set()

            if os.path.exists(output_file_path):
                try:
                    with open(output_file_path, 'r', encoding='utf-8') as f:
                        qa_dataset = json.load(f)

                    # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
                    for item in qa_dataset:
                        if 'source_index' in item:
                            processed_indices.add(item['source_index'])

                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(qa_dataset)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö Q&A –ø–∞—Ä")
                    logger.info(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏: {sorted(list(processed_indices))}")

                    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º, —Å –∫–∞–∫–æ–π —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å
                    if processed_indices:
                        last_processed = max(processed_indices)
                        if start_from <= last_processed:
                            start_from = last_processed + 1
                            logger.info(f"üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å —Å—Ç–∞—Ç—å–∏ {start_from}")

                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª: {e}")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if max_articles:
                end_idx = min(start_from + max_articles, len(df))
            else:
                end_idx = len(df)

            articles_to_process = df.iloc[start_from:end_idx]
            logger.info(f"–ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles_to_process)} (—Å {start_from} –ø–æ {end_idx - 1})")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π
            successful_generations = 0
            failed_generations = 0

            with tqdm(total=len(articles_to_process), desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Q&A") as pbar:
                for idx, row in articles_to_process.iterrows():
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                    if idx in processed_indices:
                        pbar.set_description(f"–ü—Ä–æ–ø—É—Å–∫ —Å—Ç–∞—Ç—å–∏ {idx} (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞)")
                        pbar.update(1)
                        continue

                    text = row['Text']
                    pbar.set_description(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ {idx}")

                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Q&A –ø–∞—Ä—ã
                    qa_pair = self.generate_qa_from_text(text)

                    if qa_pair:
                        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                        qa_pair['source_index'] = int(idx)
                        qa_pair['source_text_length'] = len(text)
                        qa_pair['generated_at'] = time.strftime('%Y-%m-%d %H:%M:%S')

                        qa_dataset.append(qa_pair)
                        successful_generations += 1

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥—É—é —Å—Ç–∞—Ç—å—é (–¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
                        if successful_generations % 1 == 0:
                            self.save_dataset(qa_dataset, output_file_path)
                            logger.info(f"–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {successful_generations} Q&A –ø–∞—Ä")
                    else:
                        failed_generations += 1
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å Q&A –¥–ª—è —Å—Ç–∞—Ç—å–∏ {idx}")

                    pbar.update(1)

                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(0.5)

            # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            self.save_dataset(qa_dataset, output_file_path)

            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {successful_generations} Q&A –ø–∞—Ä")
            logger.info(f"–û—à–∏–±–æ–∫: {failed_generations}")
            logger.info(f"–í—Å–µ–≥–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {len(qa_dataset)} Q&A –ø–∞—Ä")
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file_path}")

        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")
            raise

    def save_dataset(self, dataset: List[Dict], output_file_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ JSON —Ñ–∞–π–ª"""
        try:
            with open(output_file_path, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            raise


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    generator = WikipediaQAGenerator()

    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    csv_file = "kyrgyz_wikipedia_data.csv"
    output_file = "kyrgyz_wikipedia_qa_datasetGPU.json"

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
    start_from = 80000  # –ù–∞—á–∞—Ç—å —Å 70,000-–π —Å—Ç–∞—Ç—å–∏
    max_articles = None  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (None = –í–°–ï —Å—Ç–∞—Ç—å–∏!)

    try:
        generator.process_csv_file(
            csv_file_path=csv_file,
            output_file_path=output_file,
            start_from=start_from,
            max_articles=max_articles
        )
    except KeyboardInterrupt:
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()