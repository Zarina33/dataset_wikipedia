import json
import re
import os
from collections import Counter, defaultdict
from datetime import datetime, timedelta
import statistics
import matplotlib.pyplot as plt
import seaborn as sns


class KyrgyzQAAnalyzer:
    def __init__(self, file_path):
        self.file_path = file_path
        self.data = []
        self.analysis_results = {}

    def load_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
                first_char = f.read(1)
                f.seek(0)

                if first_char == '[':
                    # JSON –º–∞—Å—Å–∏–≤
                    self.data = json.load(f)
                else:
                    # JSONL —Ñ–æ—Ä–º–∞—Ç
                    for line in f:
                        line = line.strip()
                        if line:
                            self.data.append(json.loads(line))

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data):,} Q&A –ø–∞—Ä")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            return False

    def basic_statistics(self):
        """–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ Q&A –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        print("\nüìä –ë–ê–ó–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 60)

        total_pairs = len(self.data)
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ Q&A –ø–∞—Ä: {total_pairs:,}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ source_index
        source_indices = [item.get('source_index') for item in self.data if 'source_index' in item]
        if source_indices:
            unique_indices = len(set(source_indices))
            duplicates = len(source_indices) - unique_indices
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö source_index: {unique_indices:,}")
            if duplicates > 0:
                print(f"‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç—ã source_index: {duplicates:,}")

        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        text_lengths = [item.get('source_text_length', 0) for item in self.data if 'source_text_length' in item]
        if text_lengths:
            print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {statistics.mean(text_lengths):.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {statistics.median(text_lengths):.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {min(text_lengths)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {max(text_lengths):,} —Å–∏–º–≤–æ–ª–æ–≤")

        self.analysis_results['basic'] = {
            'total_pairs': total_pairs,
            'unique_source_indices': len(set(source_indices)) if source_indices else None,
            'duplicate_indices': len(source_indices) - len(set(source_indices)) if source_indices else None,
            'avg_source_length': statistics.mean(text_lengths) if text_lengths else None,
            'median_source_length': statistics.median(text_lengths) if text_lengths else None,
            'min_source_length': min(text_lengths) if text_lengths else None,
            'max_source_length': max(text_lengths) if text_lengths else None
        }

    def analyze_questions(self):
        """–ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–æ–≤"""
        print("\n‚ùì –ê–ù–ê–õ–ò–ó –í–û–ü–†–û–°–û–í")
        print("=" * 60)

        questions = [item.get('question', '') for item in self.data if 'question' in item]

        if not questions:
            print("–í–æ–ø—Ä–æ—Å—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        # –î–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
        question_lengths = [len(q) for q in questions]
        word_counts = [len(q.split()) for q in questions]

        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞: {statistics.mean(question_lengths):.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞: {statistics.median(question_lengths):.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–°–∞–º—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π –≤–æ–ø—Ä–æ—Å: {min(question_lengths)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–°–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {max(question_lengths)} —Å–∏–º–≤–æ–ª–æ–≤")

        print(f"\n–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –≤–æ–ø—Ä–æ—Å–µ: {statistics.mean(word_counts):.1f}")
        print(f"–ú–µ–¥–∏–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {statistics.median(word_counts):.1f}")

        # –ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        question_words = []
        question_patterns = {
            '—ç–º–Ω–µ': '—á—Ç–æ',
            '–∫–∞–π–¥–∞': '–≥–¥–µ',
            '–∫–∞—á–∞–Ω': '–∫–æ–≥–¥–∞',
            '–∫–∞–Ω–¥–∞–π': '–∫–∞–∫–æ–π',
            '–∫–∞–Ω—á–∞': '—Å–∫–æ–ª—å–∫–æ',
            '–∫–∏–º': '–∫—Ç–æ',
            '—ç–º–Ω–µ–≥–µ': '–ø–æ—á–µ–º—É',
            '–∫–∞–Ω—Ç–∏–ø': '–∫–∞–∫',
            '–∂”©–Ω“Ø–Ω–¥”©': '–æ —á–µ–º',
            '–¥–µ–≥–µ–Ω': '—á—Ç–æ —Ç–∞–∫–æ–µ'
        }

        pattern_counts = Counter()
        for question in questions:
            question_lower = question.lower()
            for pattern, meaning in question_patterns.items():
                if pattern in question_lower:
                    pattern_counts[f"{pattern} ({meaning})"] += 1

        print(f"\n–¢–æ–ø-10 –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π:")
        for pattern, count in pattern_counts.most_common(10):
            percentage = (count / len(questions)) * 100
            print(f"  {pattern}: {count:,} ({percentage:.1f}%)")

        # –ü—Ä–∏–º–µ—Ä—ã –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        questions_with_length = [(q, len(q)) for q in questions]
        questions_with_length.sort(key=lambda x: x[1])

        print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã —Å–∞–º—ã—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤:")
        for q, length in questions_with_length[:3]:
            print(f"  ({length} —Å–∏–º–≤.): {q}")

        print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã —Å–∞–º—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤:")
        for q, length in questions_with_length[-3:]:
            print(f"  ({length} —Å–∏–º–≤.): {q[:100]}{'...' if len(q) > 100 else ''}")

        self.analysis_results['questions'] = {
            'avg_length': statistics.mean(question_lengths),
            'median_length': statistics.median(question_lengths),
            'min_length': min(question_lengths),
            'max_length': max(question_lengths),
            'avg_words': statistics.mean(word_counts),
            'median_words': statistics.median(word_counts),
            'question_patterns': dict(pattern_counts)
        }

    def analyze_answers(self):
        """–ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–æ–≤"""
        print("\nüí¨ –ê–ù–ê–õ–ò–ó –û–¢–í–ï–¢–û–í")
        print("=" * 60)

        answers = [item.get('answer', '') for item in self.data if 'answer' in item]

        if not answers:
            print("–û—Ç–≤–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        # –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–æ–≤
        answer_lengths = [len(a) for a in answers]
        word_counts = [len(a.split()) for a in answers]

        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {statistics.mean(answer_lengths):.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {statistics.median(answer_lengths):.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–°–∞–º—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç: {min(answer_lengths)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–°–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {max(answer_lengths)} —Å–∏–º–≤–æ–ª–æ–≤")

        print(f"\n–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ: {statistics.mean(word_counts):.1f}")
        print(f"–ú–µ–¥–∏–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {statistics.median(word_counts):.1f}")

        # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤
        sentences_per_answer = []
        for answer in answers:
            # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–ø–æ —Ç–æ—á–∫–∞–º)
            sentences = len([s for s in answer.split('.') if s.strip()])
            sentences_per_answer.append(sentences)

        print(f"\n–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ—Ç–≤–µ—Ç–µ: {statistics.mean(sentences_per_answer):.1f}")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–µ –æ—Ç–≤–µ—Ç–æ–≤
        length_ranges = {
            '–û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ (‚â§50 —Å–∏–º–≤.)': sum(1 for l in answer_lengths if l <= 50),
            '–ö–æ—Ä–æ—Ç–∫–∏–µ (51-150 —Å–∏–º–≤.)': sum(1 for l in answer_lengths if 50 < l <= 150),
            '–°—Ä–µ–¥–Ω–∏–µ (151-300 —Å–∏–º–≤.)': sum(1 for l in answer_lengths if 150 < l <= 300),
            '–î–ª–∏–Ω–Ω—ã–µ (301-500 —Å–∏–º–≤.)': sum(1 for l in answer_lengths if 300 < l <= 500),
            '–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ (>500 —Å–∏–º–≤.)': sum(1 for l in answer_lengths if l > 500)
        }

        print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –¥–ª–∏–Ω–µ:")
        for range_name, count in length_ranges.items():
            percentage = (count / len(answers)) * 100
            print(f"  {range_name}: {count:,} ({percentage:.1f}%)")

        # –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤
        answers_with_length = [(a, len(a)) for a in answers]
        answers_with_length.sort(key=lambda x: x[1])

        print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤:")
        for a, length in answers_with_length[:3]:
            print(f"  ({length} —Å–∏–º–≤.): {a}")

        print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤:")
        for a, length in answers_with_length[-3:]:
            print(f"  ({length} —Å–∏–º–≤.): {a[:150]}{'...' if len(a) > 150 else ''}")

        self.analysis_results['answers'] = {
            'avg_length': statistics.mean(answer_lengths),
            'median_length': statistics.median(answer_lengths),
            'min_length': min(answer_lengths),
            'max_length': max(answer_lengths),
            'avg_words': statistics.mean(word_counts),
            'median_words': statistics.median(word_counts),
            'avg_sentences': statistics.mean(sentences_per_answer),
            'length_distribution': length_ranges
        }

    def analyze_temporal_patterns(self):
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        print("\nüïí –ê–ù–ê–õ–ò–ó –í–†–ï–ú–ï–ù–ù–´–• –ü–ê–¢–¢–ï–†–ù–û–í")
        print("=" * 60)

        timestamps = []
        for item in self.data:
            if 'generated_at' in item:
                try:
                    timestamp = datetime.strptime(item['generated_at'], '%Y-%m-%d %H:%M:%S')
                    timestamps.append(timestamp)
                except:
                    pass

        if not timestamps:
            print("–í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        timestamps.sort()

        print(f"–ü–µ—Ä–∏–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {timestamps[0]} - {timestamps[-1]}")
        print(f"–û–±—â–∞—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {timestamps[-1] - timestamps[0]}")

        # –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–Ω—è–º
        days = defaultdict(int)
        hours = defaultdict(int)

        for ts in timestamps:
            days[ts.date()] += 1
            hours[ts.hour] += 1

        print(f"\n–î–Ω–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {len(days)}")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ Q&A –≤ –¥–µ–Ω—å: {len(timestamps) / len(days):.1f}")

        # –¢–æ–ø –¥–Ω–∏ –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        top_days = sorted(days.items(), key=lambda x: x[1], reverse=True)[:5]
        print(f"\n–¢–æ–ø-5 –¥–Ω–µ–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–π:")
        for date, count in top_days:
            print(f"  {date}: {count:,} Q&A –ø–∞—Ä")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —á–∞—Å–∞–º
        print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —á–∞—Å–∞–º –¥–Ω—è:")
        for hour in sorted(hours.keys()):
            count = hours[hour]
            percentage = (count / len(timestamps)) * 100
            print(f"  {hour:02d}:00: {count:,} ({percentage:.1f}%)")

        self.analysis_results['temporal'] = {
            'start_date': timestamps[0].isoformat(),
            'end_date': timestamps[-1].isoformat(),
            'total_days': len(days),
            'avg_per_day': len(timestamps) / len(days),
            'top_days': [(str(date), count) for date, count in top_days],
            'hourly_distribution': dict(hours)
        }

    def analyze_topics_and_entities(self):
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–º –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        print("\nüè∑Ô∏è –ê–ù–ê–õ–ò–ó –¢–ï–ú –ò –°–£–©–ù–û–°–¢–ï–ô")
        print("=" * 60)

        all_text = ""
        for item in self.data:
            question = item.get('question', '')
            answer = item.get('answer', '')
            all_text += f"{question} {answer} "

        # –ü–æ–∏—Å–∫ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π (–∑–∞–∫–∞–Ω—á–∏–≤–∞—é—Ç—Å—è –Ω–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã)
        geo_patterns = [
            r'\b\w+—Å—Ç–∞–Ω\b',  # —Å—Ç—Ä–∞–Ω—ã –Ω–∞ -—Å—Ç–∞–Ω
            r'\b\w+–∏—è\b',  # —Å—Ç—Ä–∞–Ω—ã –Ω–∞ -–∏—è
            r'\b\w+–ª—å\b',  # –≥–æ—Ä–æ–¥–∞ —Ç–∏–ø–∞ "—à–∞–∞—Ä"
            r'\b\w+–æ—Ä–¥\b',  # –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –º–µ—Å—Ç–∞
        ]

        geographic_entities = set()
        for pattern in geo_patterns:
            matches = re.findall(pattern, all_text, re.IGNORECASE)
            geographic_entities.update(matches)

        # –ü–æ–∏—Å–∫ —á–∏—Å–µ–ª –∏ –¥–∞—Ç
        numbers = re.findall(r'\b\d+\b', all_text)
        years = re.findall(r'\b(19|20)\d{2}\b', all_text)

        # –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        words = re.findall(r'\b\w{3,}\b', all_text.lower())
        word_counts = Counter(words)

        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
        stop_words = {'–∂–∞–Ω–∞', '“Ø—á“Ø–Ω', '–º–µ–Ω–µ–Ω', '–±–æ–ª–≥–æ–Ω', '—ç–º–Ω–µ', '–±–æ–ª—É–ø', '–∫—ã–ª—ã–ø',
                      '–¥–µ–ø', '–±–∞—à–∫–∞', '–æ—à–æ–ª', '–∞–Ω—ã–Ω', '–±–∏—Ä–æ–∫', '–¥–∞–≥—ã'}

        content_words = [(word, count) for word, count in word_counts.items()
                         if word not in stop_words and len(word) > 3]
        content_words.sort(key=lambda x: x[1], reverse=True)

        print(f"–ù–∞–π–¥–µ–Ω–æ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π: {len(geographic_entities)}")
        print(f"–ù–∞–π–¥–µ–Ω–æ —á–∏—Å–µ–ª: {len(set(numbers))}")
        print(f"–ù–∞–π–¥–µ–Ω–æ –≥–æ–¥–æ–≤: {len(set(years))}")

        print(f"\n–¢–æ–ø-20 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤:")
        for word, count in content_words[:20]:
            print(f"  {word}: {count:,}")

        print(f"\n–ü—Ä–∏–º–µ—Ä—ã –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π:")
        for entity in sorted(geographic_entities)[:10]:
            print(f"  {entity}")

        self.analysis_results['topics'] = {
            'geographic_entities': len(geographic_entities),
            'unique_numbers': len(set(numbers)),
            'unique_years': len(set(years)),
            'top_words': content_words[:50],
            'sample_geo_entities': list(sorted(geographic_entities)[:20])
        }

    def analyze_qa_relationship(self):
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –¥–ª–∏–Ω—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤"""
        print("\nüîó –ê–ù–ê–õ–ò–ó –°–û–û–¢–ù–û–®–ï–ù–ò–Ø –í–û–ü–†–û–°-–û–¢–í–ï–¢")
        print("=" * 60)

        qa_pairs = []
        for item in self.data:
            if 'question' in item and 'answer' in item:
                q_len = len(item['question'])
                a_len = len(item['answer'])
                qa_pairs.append((q_len, a_len))

        if not qa_pairs:
            print("Q&A –ø–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        q_lengths = [pair[0] for pair in qa_pairs]
        a_lengths = [pair[1] for pair in qa_pairs]

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–ª–∏–Ω
        if len(qa_pairs) > 1:
            correlation = statistics.correlation(q_lengths, a_lengths)
            print(f"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–ª–∏–Ω—ã –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–∞: {correlation:.3f}")

        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–ª–∏–Ω
        ratios = [a_len / q_len if q_len > 0 else 0 for q_len, a_len in qa_pairs]
        avg_ratio = statistics.mean(ratios)

        print(f"–°—Ä–µ–¥–Ω–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞ –∫ –≤–æ–ø—Ä–æ—Å—É: {avg_ratio:.2f}")
        print(f"–ú–µ–¥–∏–∞–Ω–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {statistics.median(ratios):.2f}")

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é
        ratio_categories = {
            '–û—Ç–≤–µ—Ç –∫–æ—Ä–æ—á–µ –≤–æ–ø—Ä–æ—Å–∞ (<1)': sum(1 for r in ratios if r < 1),
            '–û—Ç–≤–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–≤–µ–Ω (1-2)': sum(1 for r in ratios if 1 <= r < 2),
            '–û—Ç–≤–µ—Ç –≤ 2-3 —Ä–∞–∑–∞ –¥–ª–∏–Ω–Ω–µ–µ': sum(1 for r in ratios if 2 <= r < 3),
            '–û—Ç–≤–µ—Ç –≤ 3-5 —Ä–∞–∑ –¥–ª–∏–Ω–Ω–µ–µ': sum(1 for r in ratios if 3 <= r < 5),
            '–û—Ç–≤–µ—Ç –≤ >5 —Ä–∞–∑ –¥–ª–∏–Ω–Ω–µ–µ': sum(1 for r in ratios if r >= 5)
        }

        print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –¥–ª–∏–Ω:")
        for category, count in ratio_categories.items():
            percentage = (count / len(ratios)) * 100
            print(f"  {category}: {count:,} ({percentage:.1f}%)")

        self.analysis_results['qa_relationship'] = {
            'correlation': correlation if len(qa_pairs) > 1 else None,
            'avg_ratio': avg_ratio,
            'median_ratio': statistics.median(ratios),
            'ratio_distribution': ratio_categories
        }

    def estimate_tokens(self):
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        print("\nüî¢ –û–¶–ï–ù–ö–ê –¢–û–ö–ï–ù–û–í")
        print("=" * 60)

        total_chars = 0
        total_words = 0

        for item in self.data:
            question = item.get('question', '')
            answer = item.get('answer', '')

            total_chars += len(question) + len(answer)
            total_words += len(question.split()) + len(answer.split())

        # –†–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫—ã—Ä–≥—ã–∑—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        tokens_by_chars = total_chars // 3  # ~3 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        tokens_by_words = int(total_words * 1.3)  # ~1.3 —Ç–æ–∫–µ–Ω–∞ –Ω–∞ —Å–ª–æ–≤–æ –¥–ª—è –∞–≥–≥–ª—é—Ç–∏–Ω–∞—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤

        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars:,}")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {total_words:,}")
        print(f"–û—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ (–ø–æ —Å–∏–º–≤–æ–ª–∞–º): {tokens_by_chars:,}")
        print(f"–û—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ (–ø–æ —Å–ª–æ–≤–∞–º): {tokens_by_words:,}")
        print(f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: {(tokens_by_chars + tokens_by_words) // 2:,}")

        # –¢–æ–∫–µ–Ω—ã –Ω–∞ –æ–¥–Ω—É Q&A –ø–∞—Ä—É
        avg_tokens_per_pair = (tokens_by_chars + tokens_by_words) // 2 // len(self.data)
        print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ Q&A –ø–∞—Ä—É: {avg_tokens_per_pair}")

        self.analysis_results['tokens'] = {
            'total_chars': total_chars,
            'total_words': total_words,
            'estimated_tokens_chars': tokens_by_chars,
            'estimated_tokens_words': tokens_by_words,
            'avg_estimated_tokens': (tokens_by_chars + tokens_by_words) // 2,
            'avg_tokens_per_pair': avg_tokens_per_pair
        }

    def generate_summary_report(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç"""
        print("\n" + "=" * 80)
        print("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ü–û –ö–´–†–ì–´–ó–°–ö–û–ú–£ Q&A –î–ê–¢–ê–°–ï–¢–£")
        print("=" * 80)

        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {current_time}")
        print(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö: {os.path.basename(self.file_path)}")
        print(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(self.file_path):,} –±–∞–π—Ç")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ JSON
        report_file = f"kyrgyz_qa_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.analysis_results['metadata'] = {
            'analysis_date': current_time,
            'source_file': self.file_path,
            'file_size_bytes': os.path.getsize(self.file_path),
            'analyzer_version': '1.0'
        }

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)

        print(f"\nüìÑ –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {report_file}")

        # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞
        basic = self.analysis_results.get('basic', {})
        questions = self.analysis_results.get('questions', {})
        answers = self.analysis_results.get('answers', {})
        tokens = self.analysis_results.get('tokens', {})
        temporal = self.analysis_results.get('temporal', {})

        print(f"\nüìä –ö–†–ê–¢–ö–ê–Ø –°–í–û–î–ö–ê:")
        print(f"  üî¢ –í—Å–µ–≥–æ Q&A –ø–∞—Ä: {basic.get('total_pairs', 0):,}")
        print(f"  ‚ùì –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞: {questions.get('avg_length', 0):.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  üí¨ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {answers.get('avg_length', 0):.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  üî¢ –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {tokens.get('avg_estimated_tokens', 0):,}")

        if temporal:
            print(f"  üìÖ –ü–µ—Ä–∏–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {temporal.get('total_days', 0)} –¥–Ω–µ–π")
            print(f"  ‚ö° –°—Ä–µ–¥–Ω–µ–µ Q&A –≤ –¥–µ–Ω—å: {temporal.get('avg_per_day', 0):.1f}")

        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø:")
        avg_length = basic.get('total_pairs', 0)
        if avg_length > 50000:
            print("  ‚Ä¢ –î–∞—Ç–∞—Å–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")
        if questions.get('avg_length', 0) > 50:
            print("  ‚Ä¢ –í–æ–ø—Ä–æ—Å—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        if answers.get('avg_length', 0) > 100:
            print("  ‚Ä¢ –û—Ç–≤–µ—Ç—ã —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é")

        print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")

    def run_full_analysis(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ Q&A –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        if not self.load_data():
            return

        self.basic_statistics()
        self.analyze_questions()
        self.analyze_answers()
        self.analyze_temporal_patterns()
        self.analyze_topics_and_entities()
        self.analyze_qa_relationship()
        self.estimate_tokens()
        self.generate_summary_report()


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É JSON —Ñ–∞–π–ª—É
    file_path = "/home/zarina/–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª/data_gemma/merged_array.json"

    if not file_path:
        # –ò—â–µ–º —Ñ–∞–π–ª—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        json_files = [f for f in os.listdir('.') if f.endswith(('.json', '.jsonl')) and 'kyrgyz' in f.lower()]
        if json_files:
            file_path = json_files[0]
            print(f"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {file_path}")
        else:
            print("–î–æ—Å—Ç—É–ø–Ω—ã–µ JSON —Ñ–∞–π–ª—ã:")
            for f in os.listdir('.'):
                if f.endswith(('.json', '.jsonl')):
                    print(f"  - {f}")
            exit()

    if not os.path.exists(file_path):
        print(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    else:
        analyzer = KyrgyzQAAnalyzer(file_path)
        analyzer.run_full_analysis()